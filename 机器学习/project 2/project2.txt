单词计数向量，在开始分类之前，必须先将文本编码成数字
一种常用的方法就是单词计数向量，在这种技术中，一个样本可以包含一段话或者一篇文章，这个样本如果出现了10个单词，就会有10个特征，每个特征代表一个单词，特征的取值代表这个单词在这个样本中总共出现了几次，是一个离散的，代表次数的整数。
在sklearn中，单词计数向量计数可以通过feature_extraction.text模块中的CountVectorizer类实现.

##实际上那个直接可以应用feature_extraction.text模块中的CountVectorizer类实现
可以预见，如果使用单词计数向量，可能会导致一部分常用词频繁出现在矩阵中并且占有很高的权重，对分类来说，这明显是对算法的一种误导。为了解决这个问题，比起使用次数，使用单词在句子中所占的比例来编码单词，这就是TF-IDF方法，词频逆文档频率，是通过单词在文档中出现的频率来衡量其权重，IDF的大小与一个词的常见程度成反比，这个词越常见，编码后为它设置的权重就会倾向于越小，从而来压制频繁出现的一些无意义的词
在sklearn中，使用feature_extraction.text中类TfidVectorizer来执行这种编码


朴素贝叶斯
有各种各样的假设，除了“朴素”的假设即假设变量之间相互独立的假设，还有对于概率分布的假设，包括多项式朴素贝叶斯 multinomialNB(二项分布、多项分布，擅长分类型变量，多项式朴素贝叶斯的特征矩阵经常是稀疏矩阵，所以经常被用于文本分类)、伯努利朴素贝叶斯（二项分布，数据集中存在多个特征，但每个特征都是二分类的，可以用布尔变量表示，由于本数据集中有20个类，那么可以使用类中专门用来二值话的参数binarize来改变数据）、高斯朴素贝叶斯、


多项式朴素贝叶斯的效果不太理想，如果采用哑变量方式的分箱处理，多项式
伯努利朴素贝叶斯和多项式朴素贝叶斯非常相似，常用于处理文本分类数据，但由于伯努利朴素贝叶斯处理二项分布，所以更在意存在与否，而不是出现多少次，这是两者的根本不同。
在文本分类的情况下，伯努利朴素贝叶斯可以使用单词出现向量而不是单词计数向量来训练分类器，在文档较短的数据集上，伯努利朴素贝叶斯效果更好

补集在样本不均衡中很好


超参数：平滑系数 alpha，是为了防止训练数据中出现过的一些词汇没有出现在测试集中导致0概率，如果alpha为1，则这个平滑叫做拉普拉斯平滑，如果alpha


输入的矩阵是稀疏矩阵






ROC曲线是不能用于多分类的，多分类状况下的最佳模型评估指标是混淆矩阵和整体的准确度
1.导入需要的库和数据
2.归一化，确保输入的矩阵不带有负数
3.建模，探索建模结果
4.使用混淆矩阵来查看贝叶斯的分类结果
常用的三种数据分布：月亮型，环形数据和二分类数据
高斯朴素贝叶斯其分类效果在月亮型和二分类数据比较好